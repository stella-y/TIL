다음 스터디 방향을 PySpark로 어떨지 간보기 위해 선정한 Chapter입니다. 근데 아쉽게도 PySpark는 한장이고 R Spark 내용과 예시들이 대부분


1.PySpark


1.1 PySpark의 근본적인 차이점
구조적 APIs:
Python UDFs가 아니라면 비슷한 빠르기


저수준APIs(특히 RDD):
유연성을 얻는대신 성능 저하를 감수해야 함 (Python 데이터를 스파크와 JVM에서 이해할 수 있도록 변환하고 그 반대로 변환하는 과정에서 큰 비용이 발생하기 때문. 직렬화 처리 과정과 함수처리 과정이 포함)



아래사진은 천만개의 integer pairs에 대해 group-by-aggregation했을때 시간 (주의: 5년 전 실험)

출처

1.2 Pandas 통합하기
장점: 스파크로 빅데이터처리 그 후 Pandas로 작은 데이터 처리

(예: 스파크로 ETL후 적당히 작은 결과를 드라이버로 수집. 그 후 Pandas로 추가 처리작업 진행)



import pandas as pd
df = pd.Dataframe({"first": range(200), "second": range(50, 250)})
sparkDF = spark.createDataFrame(df)
newPDF = sparkDF.toPandas() newPDF.head()


UDF벡터화 관련:

개별 로우를 Python object로 변환하는 대신, Python을 이용해 스파크 DataFrame을 Pandas 라이브러리의 DataFrame 시리즈로 처리할 수 있게됨.

2.3에 포함됨 : https://issues.apache.org/jira/browse/SPARK-21190


**책 내용 아님** (출처)
JVM<-> Python 변환을 optimize해주는 Apache Arrow라는 데이터포맷도 존재

"Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer data between JVM and Python processes"
"Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer data and Pandas to work with the data.
A Pandas UDF is defined using the keyword pandas_udf as a decorator or to wrap the function, no additional configuration is required.
Currently, there are two types of Pandas UDF: Scalar and Grouped Map."

PySpark  UDF 예시 

from pyspark.sql.functions import col, pandas_udf
from pyspark.sql.types import LongType
 
# I: pd.Series, O: pd.Series
def multiply_func(a, b):
    return a * b
 
multiply = pandas_udf(multiply_func, returnType=LongType())
#내부적으로 column을 여러개의 batch로 나눠서 하나씩 처리 후 concat해서 나온다고 함.
 
#이미 알다시피 local pandas데이터는 다음과 같이 처리할 수 있다
x = pd.Series([1, 2, 3])
print(multiply_func(x, x))
# 0    1
# 1    4
# 2    9
# dtype: int64
 
 
df = spark.createDataFrame(pd.DataFrame(x, columns=["x"]))
 
# Spark vectorized UDF로서 처리하기
df.select(multiply(col("x"), col("x"))).show()
# +-------------------+
# |multiply_func(x, x)|
# +-------------------+
# |                  1|
# |                  4|
# |                  9|
# +-------------------+
 
 
 
 
# Grouped Map 예시
# 1.DataFrame.groupBy로 group나눔
# 2.각각의 group에 udf 적용. udf 의 I, O 은 역시 pandas.DataFrame
# 3. 2의 결과를 DataFrame으로 뭉치기
 
 
from pyspark.sql.functions import pandas_udf, PandasUDFType
 
df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], ("id", "v"))
 
@pandas_udf("id long, v double", PandasUDFType.GROUPED_MAP)
def subtract_mean(pdf):
    v = pdf.v
    return pdf.assign(v=v - v.mean())
 
df.groupby("id").apply(subtract_mean).show()
# +---+----+
# | id|   v|
# +---+----+
# |  1|-0.5|
# |  1| 0.5|
# |  2|-3.0|
# |  2|-1.0|
# |  2| 4.0|
# +---+----+
 
 
# 그 외, 제한적이지만Grouped Aggregate도 있음


PySpark RDD예시(출처: https://data-flair.training/blogs/pyspark-rdd/)

from pyspark import SparkContext
sc = SparkContext("local", "어쩌구 app")
words = sc.parallelize (["scala","java","hadoop","spark","akka","spark vs hadoop","pyspark","pyspark and spark"])
 
 
 
 
---foreach(f)---
def f(x): print(x)
    fore = words.foreach(f)   # scala, ...
 
 
 
 
---cc---
words_filter = words.filter(lambda x: 'spark' in x)
filtered = words_filter.collect() # [‘spark’, ..]
 
 
---map(f,preservesPartitioning=False)
words_map = words.map(lambda x: (x, 1))
mapping = words_map.collect()  # [(‘scala’, 1),...]
 
 
--reduce(f)--
from pyspark import SparkContext
from operator import add
sc = SparkContext("local", "저쩌구 app")
nums = sc.parallelize([1, 2, 3, 4, 5])
adding = nums.reduce(add) # 15
 
 
--join(other, numPartitions=None)--
x = sc.parallelize([("spark", 1), ("hadoop", 4)])
y = sc.parallelize([("spark", 2), ("hadoop", 5)])
joined = x.join(y)
final = joined.collect()  #[(‘spark’, (1, 2)),(‘hadoop’, (4, 5))]
 
 
--cache--
words.cache()
caching = words.persist().is_cached # true






2.R로 스파크 사용하기
**내용이 이것저것 맛보기 위주**



스파크는 R을 지원하기 위해 두 가지 핵심 컴포넌트 제공: SparkR, sparklyr

SparkR은 R의 data.frame과 유사한 DataFrame API 제공
sparklyr는 구조적 데이터에 접근하기 위해 dplyr패키지를 사용.
추후 SparkR과 sparklyr는 하나로 통합될 것으로 예상



 sparkR vs. sparklyr


2.1 SparkR
SparkR의 장단점
다음과 같은 경우, PySpark 대신 SparkR을 사용:

R 언어에 익숙하고 스파크의 기능을 조금씩 활용하고 싶은 경우
R 고유의 기능이나 ggplot2와 같이 훌륭한 라이브러리를 활용해 프로세스 과정 중에 빅데이터를 처리하고 싶은 경우


#예시
retail.data <- read.df("~", "csv", header="true", inferSchema="true")
print(str(retail.data))
# csv파일을 읽은 SparkDataFrame의 일부 데이터를 표준 R data.frame타입으로 변환할 수 있음
local.retail.data <- take(retail.data, 5)
print(str(local.retail.data))


핵심 개념
PySpark에서 사용한 기능을 SparkR에서 사용할 수 있습니다. 단 RDD 등의 저수준 API를 지원하지 않음.
R 데이터 타이봐 스파크 타입의 차이점. data.frame타입과 스파크 타입의 가장 큰 차이점은 data.frame 타입은 로컬 메모리상에 데이터가 존재하고 특정 로컬 프로세스에서 직접 사용 가능.


#스파크에서 로컬 개발 환경으로 데이터를 수집
collect(count(groupBy(retail.data, "country")))
함수 마스킹
스파크에서 특정 함수를 마스킹함
SparkDataFrames의 고유 함수
SparkR에선 함수 마스킹의 영향으로 로컬 객체를 사용하던 함수가 동작하지 않을 수 있음.
sample(mtcars) #오류발생. 스파크가 sample이라는 함수명 이미 있기때문
#스파크가 sample이라는 함수명을 가지고 있기 때문에 표준 data.frame의 sample 함수를 사용할 수 없음.
#data.frame의 sample함수를 사용하기 위해서는 기본 sample 함수를 명시적으로 사용해야 함.
base::sample(some.r.data.frame) # some.r.data.frame = R data.frame type
데이터 처리
tbls <- sql("SHOW TABLES")
collect(select(filter(tbls, like(tbls$tableName, "%production%")),"tableName","isTemporary"))
데이터소스
SparkR은 스파크와 3rd party 패키지의 모든 데이터소스 지원
머신러닝
빈약. 모델 학습이나 머신러닝 알고리즘 활용해야하는 경우 스파크 잘 쓰지 않음. 
큰 규모의 데이터에서 학습셋을 추출하기 우해 스파크를 사용한 후 로컬 환경으로 옮겨와 data.frame으로 학습.
UDF
SparkR의 UDF는 JVM 내외부에서 함수를 직렬화 하는 파이썬 사용자 정의 함수와 같은 방식으로 동작
스파크의 spark.lapply, dapply&dapplyCollect, gapply&gapplyCollect 함수를 이용하여 UDF만들수 있음. 여기서 예시는 생략 (p. 742, 743, 744)




2.2 sparklyr


dplyr를 기반으로 RStudio 팀에서 만든 최신 패키지.



핵심 개념
SparkSession 사용하는 대신 spark_connect를 사용하여 스파크 클러스터에 접속



DataFrame 사용불가
대신 다른 dplyr 데이터소스와 유사한 테이블(스파크 내부의 DataFrame에 매핑됨)을 데이터 처리에 활용

스파크에 내장된 함수나 API를 dplyr에서 지원하지 않으면 사용불가



데이터 처리
dplyr의 로컬 data.frame을 다룰 때처럼 지원하는 모든 dplyr함수와 기능 사용 가능

UDF는 지원 안함 (SparkR에서는 dapply, gapply, lappy를 사용해 UDF를 만들고 적용할 수 있음)



SQL실행
DBI 라이브러리를 사용하여 SQL코드를 클러스터에서 실행.

libary(DBI) allTables <- dbGetQuery(sc, "SHOW TABLES") #스파크 SQL과 관련된 전용 속성값을 스파크 클러스터에 지정할 수 있음. 그 외 스파크 전용 속성 설정X
setShufflePartitions <- dbGetQuery(sc, "SET spark.sql.shuffle.partitions=10")
데이터소스
연산결과를 파일 포맷으로 저장할 때 csv, json, parquet만 지원



3 정리
SparkR & sparklyr 는 급성장하고 있는 분야